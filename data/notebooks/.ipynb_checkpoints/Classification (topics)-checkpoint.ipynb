{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4859863b-8853-440a-9dee-24d710658907",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "   AutoConfig,\n",
    "   AutoTokenizer,\n",
    "   TFAutoModelForSequenceClassification,\n",
    "   AdamW,\n",
    "   glue_convert_examples_to_features\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67e603dc-70d0-4ced-a856-7b95945f7576",
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "tyLIi4bvImJZ"
   },
   "outputs": [],
   "source": [
    "# Choose model\n",
    "# @markdown >The default model is <i><b>COVID-Twitter-BERT</b></i>. You can however choose <i><b>BERT Base</i></b> or <i><b>BERT Large</i></b> to compare these models to the <i><b>COVID-Twitter-BERT</i></b>. All these three models will be initiated with a random classification layer. If you go directly to the Predict-cell after having compiled the model, you will see that it still runs the predition. However the output will be random. The training steps below will finetune this for the specific task. <br /><br /> \n",
    "model_name = 'digitalepidemiologylab/covid-twitter-bert-v2' #@param [\"digitalepidemiologylab/covid-twitter-bert\", \"bert-large-uncased\", \"bert-base-uncased\"]\n",
    "\n",
    "# Initialise tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "146d3a03-e38a-4534-abbf-4b35a7b15f50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Quarantine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SHAHZE~1\\AppData\\Local\\Temp/ipykernel_3052/3480467552.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mtrain_label\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Quarantine'"
     ]
    }
   ],
   "source": [
    "# Training Paremeters\n",
    "max_seq_length = 128 #@param {type: \"integer\"}\n",
    "train_batch_size =  8 #@param {type: \"integer\"} \n",
    "eval_batch_size = 8 #@param {type: \"integer\"}\n",
    "num_labels = 2\n",
    "\n",
    "# Loading the Training dataset\n",
    "t_train = pd.read_csv(\"..\\\\raw\\\\cluster_input.csv\", names=('text', 'label', 'topic', 'topic_label'))\n",
    "#t_train = t_train.sample(n=30000)\n",
    "\n",
    "#t_train[\"text\"] = t_train[\"text\"].apply(remove_contractions)\n",
    "#t_train[\"text\"] = t_train[\"text\"].apply(clean_text)\n",
    "#t_train.drop_duplicates(subset=[\"text\"], inplace=True)\n",
    "#t_train.dropna(inplace=True)\n",
    "\n",
    "#X_train = t_train[\"text\"][:len(df)*0.64]\n",
    "#X_val = t_train[\"text\"][len(df)*0.64:len(df)*0.8]\n",
    "#X_test = t_train[\"text\"][len(df)*0.8:]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(t_train['text'], t_train['topic_label'], test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=1) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "train_text = []\n",
    "val_text = []\n",
    "test_text = []\n",
    "\n",
    "train_label = []\n",
    "val_label = []\n",
    "test_label = []\n",
    "\n",
    "for row in X_train:\n",
    "    train_text.append(str(row))\n",
    "    \n",
    "for row in X_val:\n",
    "    val_text.append(str(row))\n",
    "    \n",
    "for row in X_test:\n",
    "    test_text.append(str(row))\n",
    "    \n",
    "for row in y_train:\n",
    "    train_label.append(int(row))\n",
    "\n",
    "for row in y_val:\n",
    "    val_label.append(int(row))\n",
    "    \n",
    "for row in y_test:\n",
    "    test_label.append(int(row))\n",
    "\n",
    "#train_text = tokenizer(train_text, max_length=max_seq_length, truncation=True, padding=True)\n",
    "#val_text = tokenizer(val_text, max_length=max_seq_length, truncation=True, padding=True)\n",
    "#test_text = tokenizer(test_text, max_length=max_seq_length, truncation=True, padding=True)\n",
    "\n",
    "train = np.zeros([np.size(train_text), max_seq_length], dtype=int)\n",
    "val = np.zeros([np.size(val_text), max_seq_length], dtype=int)\n",
    "\n",
    "for i in range(len(train_text)):\n",
    "    tokens = np.asarray(tokenizer.encode(train_text[i], max_length=max_seq_length, truncation=True, padding=True))\n",
    "    for j in range(np.size(tokens)):\n",
    "        train[i][j] = tokens[j]\n",
    "\n",
    "for i in range(len(val_text)):\n",
    "    tokens = np.asarray(tokenizer.encode(val_text[i], max_length=max_seq_length, truncation=True, padding=True))\n",
    "    for j in range(np.size(tokens)):\n",
    "        val[i][j] = tokens[j]\n",
    "\n",
    "\n",
    "#a = tokenizer.encode(train_text[0], max_length=max_seq_length, truncation=True, padding=True)\n",
    "#print(np.shape(a))\n",
    "\n",
    "\n",
    "#train_text = np.asarray(train_text)\n",
    "#val_text = np.asarray(val_text)\n",
    "#test_text = np.asarray(test_text)\n",
    "\n",
    "train_label = np.asarray(train_label)\n",
    "val_label = np.asarray(val_label)\n",
    "test_label = np.array(test_label)\n",
    "\n",
    "label_mapping = {0:'Masks', 1:'Vaccine', 2:'Symptoms', 3:'Quarantine', 4:'Lockdown', 5:'Education', 6:'Treatment',\n",
    "                 7:'Science', 8:'Statistics', 9:'Health', 10:'Work', 11:'Legislation', 12:'Politics', 13:'Travel', 14:'Testing'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9652e-70b3-416d-bfee-870f3e990cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b241b7f-0c49-48e4-ac4f-f6a6f9472735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_t = tf.convert_to_tensor(train_text)\n",
    "#val_t = tf.convert_to_tensor(val_text)\n",
    "#test_t = tf.convert_to_tensor(test_text)\n",
    "#train_l = tf.convert_to_tensor(train_label)\n",
    "#val_l = tf.convert_to_tensor(val_label)\n",
    "#test_l = tf.convert_to_tensor(test_label)\n",
    "print(np.shape(train_text))\n",
    "\n",
    "np.shape(train)\n",
    "np.shape(train_label)\n",
    "\n",
    "#print(np.shape(train[0]))\n",
    "#print(np.shape(train[5]))\n",
    "print(train[0])\n",
    "print(train[5])\n",
    "#print(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c906ec-f8ac-48bc-9526-4a805abca863",
   "metadata": {
    "cellView": "both",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 695,
     "referenced_widgets": [
      "448f914b46b644f5851f1af379b6a4e1",
      "0b613b02dfe34bb682b4964c15808aff",
      "fe8a58e6570b47368d8af58c9f88252d",
      "def365577baa4c06964afa044aeaf6c2",
      "f0265fb7ba1b45f6a9f9187918bf549c",
      "d299f05a0190420cbee345284d7018a0",
      "3554e30180a0456f9f3aa5a08ebe1cdb",
      "299748a6b86041ab8dbdddef56b8a1c1"
     ]
    },
    "colab_type": "code",
    "id": "2XSbsZFDQTEO",
    "outputId": "2596ac65-32e7-4a2e-ef9b-5aaf2689d969"
   },
   "outputs": [],
   "source": [
    "#@markdown >The default learning rate of 2e5 will be fine in most cases\n",
    "learning_rate = 2e-5 #@param {type: \"number\"}\n",
    "\n",
    "#@markdown > Typically these type of models are finetuned for 3 epochs. This can be increased for small datasets and decreased for large datasets.\n",
    "num_epochs = 3 #@param {type: \"integer\"}\n",
    "\n",
    "# Initialise a Model for Sequence Classification with 2 labels\n",
    "config = AutoConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "#loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# Metrics and callbacks no false negatives bitch\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
    "#metrics = [tf.keras.metrics.Recall(), tf.keras.metrics.Precision()]\n",
    "#checkpoint_path = '..\\\\cvb\\\\checkpoints\\\\checkpoint.{epoch:02d}'\n",
    "#callbacks = [tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True)]\n",
    "\n",
    "# Compute some variables\n",
    "use_percentage_of_data = 100\n",
    "train_steps_per_epoch = int(np.size(train[:,0]) * (use_percentage_of_data/100) / train_batch_size)\n",
    "dev_steps_per_epoch = int(np.size(val[:,0]) * (use_percentage_of_data/100) / eval_batch_size)\n",
    "#train_steps_per_epoch = 100\n",
    "#dev_steps_per_epoch = 100\n",
    "\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train, train_label, epochs=num_epochs, steps_per_epoch=train_steps_per_epoch, validation_data=(val, val_label), validation_steps=dev_steps_per_epoch, batch_size=8)\n",
    "\n",
    "'''\n",
    "# Print some information about the training\n",
    "print(f'\\nThe training has finished training after {num_epochs} epochs.')\n",
    "print('\\nThe history contains the accuracy and loss at every epoch:')\n",
    "print(json.dumps(history.history, indent=4))\n",
    "\n",
    "print('\\nThe checkpoint callback has generated a checkpoint after every epoch (loss being the training loss, val_loss is the validation loss):')\n",
    "!ls -lha ./checkpoints/\n",
    "\n",
    "print('\\nWe will now save the finetuned model and the corresponding config file on your Colab disk.')\n",
    "model.save_pretrained('..\\\\cvb\\\\huggingface_model\\\\')\n",
    "\n",
    "print('\\nTensorflow model and config-file is saved in ./huggingface_model/')\n",
    "!ls -lha ./huggingface_model/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134fa73-9b35-466d-9c87-c02d73870379",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0T0zy0fK9Rmx"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Small function only used for formatting the output\n",
    "def format_prediction(preds, label_mapping, label_name):\n",
    "    preds = tf.nn.softmax(preds, axis=1)\n",
    "    formatted_preds = []\n",
    "    for pred in preds.numpy():\n",
    "        # convert to Python types and sort\n",
    "        pred = {label: float(probability) for label, probability in zip(label_mapping.values(), pred)}\n",
    "        pred = {k: v for k, v in sorted(pred.items(), key=lambda item: item[1], reverse=True)}\n",
    "        formatted_preds.append({label_name: list(pred.keys())[0], f'{label_name}_probabilities': pred})\n",
    "    return formatted_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce36768-cbee-4a97-8dff-2bc452b2b9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def evaluation_summary(description, predictions, true_labels):\n",
    "    print(\"Evaluation for: \" + description)\n",
    "    precision = precision_score(predictions, true_labels, average='macro')\n",
    "    recall = recall_score(predictions, true_labels, average='macro')\n",
    "    accuracy = accuracy_score(predictions, true_labels)\n",
    "    f1 = f1_score(predictions, true_labels, average='macro')\n",
    "    #f1 = fbeta_score(predictions, true_labels, 1, average='macro') #1 means f_1 measure\n",
    "    print(\"Classifier '%s' has Acc=%0.3f P=%0.3f R=%0.3f F1=%0.3f\" % (description,accuracy,precision,recall,f1))\n",
    "    print(classification_report(predictions, true_labels, digits=3, zero_division=0))\n",
    "    print('\\nConfusion matrix:\\n',confusion_matrix(true_labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bf98b1-48ee-4958-9acd-cb24ba552b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.zeros([np.size(test_text), max_seq_length], dtype=int)\n",
    "\n",
    "for i in range(len(test_text)):\n",
    "    tokens = np.asarray(tokenizer.encode(test_text[i], max_length=max_seq_length, truncation=True, padding=True))\n",
    "    for j in range(np.size(tokens)):\n",
    "        test[i][j] = tokens[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce37235-5192-453f-bedf-e3a687fbf079",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb8bd0-1dac-4288-ae1e-b915f3b8e58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_test_preds = format_prediction(test_pred[0], label_mapping, 'fake?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c29846-866c-4d5a-b85e-2bde29397fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_labels = []\n",
    "for i in range(len(test_text)):\n",
    "    if (formatted_test_preds[i]['fake?_probabilities']['Real'] > 0.5):\n",
    "        test_pred_labels.append(0)\n",
    "    else:\n",
    "        test_pred_labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c481cc69-8300-4e4c-a56c-379b355a0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_summary(\"Covid-Twitter-Bert-v2\", test_pred_labels, test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
